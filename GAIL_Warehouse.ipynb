{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# LIBRERIAS NECESARIAS\n",
    "#########################################################################\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import sinergym\n",
    "from sinergym.utils.wrappers import LoggerWrapper\n",
    "import torch\n",
    "\n",
    "import tensorflow as tf\n",
    "from stable_baselines3 import PPO\n",
    "from tensorflow.keras.layers import concatenate\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Variables globales\n",
    "###########################################################################\n",
    "BATCH_SIZE = 28032\n",
    "EPOCHS =100\n",
    "EPISODES = 1\n",
    "EPISODES_EVALUATE_G=5\n",
    "TOTAL_TIMESTEPS_PPO_GENERATOR=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el entorno Warehouse\n",
    "env = gym.make('Eplus-warehouse-hot-discrete-v1')\n",
    "env = LoggerWrapper(env)\n",
    "\n",
    "# Obtenemos el espacio de observaciones y el espacio de acciones del entorno env\n",
    "ob_space = env.observation_space\n",
    "ac_space = env.action_space\n",
    "\n",
    "# Mostramos el número de observaciones y de acciones del entorno 5Zone\n",
    "print('\\n Nº de observaciones: ', ob_space.shape[0])\n",
    "print('\\n Nº de acciones: ', ac_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disriminador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal del Discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "# Red neuronal del Discriminador\n",
    "################################################################################################################################################\n",
    "\n",
    "# Input: secuencias [s,a] reales o falsas, de longitud 2*ob_space.shape[0] + ac_space.n+1. \n",
    "# Output: probabilidad de real o falso perteneciente al intervalo [0,1]\n",
    "discriminator_net = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(None, 2*ob_space.shape[0] + ac_space.n+1)),\n",
    "        layers.Dense(units=17, activation=tf.nn.relu, name='layer1'),\n",
    "        layers.Dense(units=17, activation=tf.nn.relu, name='layer2'),\n",
    "        layers.Dense(units=17, activation=tf.nn.relu, name='layer3'),\n",
    "        layers.Dense(units=1, activation=tf.sigmoid, name='prob'),\n",
    "\n",
    "    ],\n",
    "    name=\"discriminator_net\"\n",
    "\n",
    ")\n",
    "discriminator_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de pérdida del Discriminador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "# Función de pérdida del Discriminador\n",
    "#########################################################################################################\n",
    "\n",
    "# prob1=> output de la red neuronal del Discriminador cuando recibe como entrada una secuencia REAL [s,a] de la base de datos \n",
    "# prob2=> output de la red neuronal del Discriminador cuando recibe como entrada una secuencia FALSA [s,a] \n",
    "def loss_fn_D(prob1, prob2):\n",
    "\n",
    "    # Esperanza del logaritmo de la D(x)=salida de la red neuronal cuando x=entrada REAL\n",
    "    loss_expert = tf.reduce_mean(tf.math.log(tf.clip_by_value(prob1, 0.01, 1)))\n",
    "    \n",
    "    # Esperanza del logaritmo de 1-D(x) donde D(x)=salida de la red neuronal cuando x=entrada FALSA\n",
    "    loss_agent = tf.reduce_mean(tf.math.log(tf.clip_by_value(1 - prob2, 0.01,1)))\n",
    "    \n",
    "    loss_expert = tf.cast(loss_expert, dtype=tf.float32)\n",
    "    loss_agent = tf.cast(loss_agent, dtype=tf.float32)\n",
    "    \n",
    "    loss = loss_expert + loss_agent\n",
    "    \n",
    "    loss = -loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase del Discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self,env,discriminator_net,expert_s,expert_a,agent_s,agent_a):\n",
    "\n",
    "        # -Red neuronal del Discriminador\n",
    "        self.discriminator_net = discriminator_net\n",
    "\n",
    "        # 1) Secuencia experta: [s,a]\n",
    "        self.expert_s = expert_s\n",
    "        self.expert_a = expert_a\n",
    "        expert_a_one_hot = tf.one_hot(self.expert_a, depth=env.action_space.n)\n",
    "        # Añadimos ruido para estabilizar el entrenamiento\n",
    "        expert_a_one_hot += tf.random.normal(tf.shape(expert_a_one_hot),mean=0.2,stddev=0.1,dtype=tf.float32) / 1.2\n",
    "        # expert_s_a=>secuencia experta=>[s,a]\n",
    "        self.expert_s_a = tf.concat([self.expert_s, expert_a_one_hot], axis=1)\n",
    "\n",
    "        # 2) Secuencia agente:  [s,a]\n",
    "        self.agent_s = agent_s\n",
    "        self.agent_a = agent_a\n",
    "        agent_a_one_hot = tf.one_hot(self.agent_a, depth=env.action_space.n)\n",
    "        agent_a_one_hot += tf.random.normal(tf.shape(agent_a_one_hot),mean=0.2,stddev=0.1,dtype=tf.float32) / 1.2\n",
    "        # agent_s_a=>secuencia agente=>[s,a]\n",
    "        self.agent_s_a = tf.concat([self.agent_s, agent_a_one_hot], axis=1)\n",
    "\n",
    "        # Calculamos la salida de la red para [s,a] del experto y [s,a] del agente \n",
    "\n",
    "        # -Salida de la red neuronal Discriminador para [s,a] expertos(verdaderos)\n",
    "        self.prob_expert = self.discriminator_net(self.expert_s_a)\n",
    "\n",
    "        # -Salida  de la red neuronal Discrimiinador para [s,a] Agente(falsos)\n",
    "        self.prob_agent = self.discriminator_net(self.agent_s_a)\n",
    "\n",
    "        # -Recompensa obtenida cuando el Agente realiza [s,a] falsas\n",
    "        self.rewards = tf.math.log(tf.clip_by_value(self.prob_agent, 1e-10, 1))\n",
    "\n",
    "    def getNet(self):\n",
    "        return self.discriminator_net\n",
    "\n",
    "    def getAgent_S_A(self):\n",
    "        return self.agent_s_a\n",
    "\n",
    "    def getExpert_S_A(self):\n",
    "        return self.expert_s_a\n",
    "\n",
    "    def getProb(self):\n",
    "        return self.prob_expert, self.prob_agent\n",
    "\n",
    "    def getRewards(self):\n",
    "        return self.rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales del Generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red neuronal del Generador donde se producen acciones\n",
    "# Input: estados, listas de tamaño 20, s=[s1,s2,s3,....,s23]\n",
    "# Output: acciones, listas de tamaño 10, a=[a1,a2,....,a10]\n",
    "generator_net_Act = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(None, ob_space.shape[0])),\n",
    "        layers.Dense(units=20, activation=tf.tanh, name='layer1'),\n",
    "        layers.Dense(units=20, activation=tf.tanh, name='layer2'),\n",
    "        layers.Dense(units=15, activation=tf.tanh, name='layer3'),\n",
    "        layers.Dense(units=ac_space.n, activation=tf.nn.softmax, name='layer4')\n",
    "\n",
    "    ],\n",
    "    name=\"generator_net_Act\"\n",
    ")\n",
    "\n",
    "generator_net_Act.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red neuronal del Generador donde se producen v_pred\n",
    "# Input: estados, listas de tamaño 20, s=[s1,s2,s3,....,s17]\n",
    "# Output: v_pred, listas de tamaño 1, v_pred(s)\n",
    "\n",
    "generator_net_v_preds = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(None, ob_space.shape[0])),\n",
    "        layers.Dense(units=20, activation=tf.tanh, name='layer1'),\n",
    "        layers.Dense(units=20, activation=tf.tanh, name='layer2'),\n",
    "        layers.Dense(units=1, activation=None, name='layer3'),\n",
    "    ],\n",
    "    name=\"generator_v_preds\"\n",
    ")\n",
    "\n",
    "generator_net_v_preds.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de pérdida del Generador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de pérdida del Generador: función objetivo de PPO \"clipped surrogated\" \n",
    "def loss_fn_ppo(act_probs,act_probs_old,gaes,clip_value=0.2):\n",
    "    ratios = tf.exp(tf.math.log(tf.clip_by_value(act_probs, 1e-10, 1.0))\n",
    "                    - tf.math.log(tf.clip_by_value(act_probs_old, 1e-10, 1.0)))\n",
    "\n",
    "    clipped_ratios = tf.clip_by_value(ratios,clip_value_min=1 -clip_value,clip_value_max=1 +clip_value)\n",
    "    loss_clip = tf.minimum( tf.multiply(gaes, ratios), tf.multiply(gaes, clipped_ratios))\n",
    "    loss_clip = tf.reduce_mean(loss_clip)\n",
    "    \n",
    "    loss = -loss_clip\n",
    "    tf.summary.scalar('total', loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase del Generador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "# Clase del GENERADOR: política con su optimizador PPO\n",
    "################################################################################################################\n",
    "\n",
    "# Observesé que cada generador implementa una política distinta, por tanto, se ha decidido llamar a la clase Policy_net en lugar de generator\n",
    "\n",
    "class Policy_net:\n",
    "    def __init__(self, name: str, env, obs):\n",
    "        \"\"\"\n",
    "        name: string\n",
    "        env: gym env\n",
    "        obs:\n",
    "        \"\"\"\n",
    "\n",
    "        # -Entorno\n",
    "        self.env = env\n",
    "        env.reset()\n",
    "\n",
    "        # -Modelo PPO: algoritmo de Optimización de Política Proximal  \n",
    "        self.model = PPO(policy=\"MlpPolicy\", env=env, verbose=0)\n",
    "\n",
    "        self.model.learn(total_timesteps=TOTAL_TIMESTEPS_PPO_GENERATOR)  # 25000)\n",
    "\n",
    "        # -Observación inicial a partir de la cual se crean las acciones iniciales haciendo uso de las redes neuronales del generador\n",
    "        self.obs = np.reshape(np.array(obs), (1, ob_space.shape[0]))\n",
    "\n",
    "        # Utilizamos las dos redes neuronales que hemos creado : generator_net_Act y generator_net_v_preds\n",
    "        # V_pred=>recompensa media de que un agente ejecute una acción\n",
    "\n",
    "        # -Acción inicial generada con red neuronal y v_pred con red neuronal\n",
    "        self.act_probs = generator_net_Act(self.obs)\n",
    "        self.v_preds = generator_net_v_preds(self.obs)\n",
    "\n",
    "        # -Accion estocástica inicial\n",
    "        self.act_stochastic = tf.random.categorical( tf.math.log(self.act_probs), num_samples=1)\n",
    "\n",
    "        # -Acción determinística inicial\n",
    "        self.act_deterministic = tf.argmax(self.act_probs, axis=1)\n",
    "\n",
    "    # Para cada estado obs me dice la acción que el agente va a ejecutar sobre el entorno junto con v_pred\n",
    "    # La elección de la acción puede ser estocástica o determinística\n",
    "\n",
    "    def act(self, stochastic=True):\n",
    "        if stochastic:\n",
    "            return self.act_stochastic, self.v_preds\n",
    "        else:\n",
    "            return self.act_deterministic, self.v_preds\n",
    "\n",
    "    def get_action_prob(self):\n",
    "        return self.act_probs\n",
    "\n",
    "    def get_v_preds(self):\n",
    "        return self.v_preds\n",
    "\n",
    "    def get_obs(self):\n",
    "        return self.obs\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def get_trainable_variables(self):\n",
    "        return self.model.get_parameters()\n",
    "\n",
    "    # Generar [s,a] falsos\n",
    "    def generate_fakes(self):\n",
    "\n",
    "        ob_space = env.observation_space\n",
    "        reward = 0\n",
    "        success_num = 0\n",
    "\n",
    "        # Por cada episodio\n",
    "        for iteration in range(EPISODES):\n",
    "            # Inicializo todas las variables\n",
    "            observations = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            run_policy_steps = 0\n",
    "\n",
    "            truncated = False\n",
    "            terminated = False\n",
    "\n",
    "            # La primera acción de cada episodio se crea con la red neuronal\n",
    "\n",
    "            obs,_ = env.reset()\n",
    "\n",
    "            Old_Policy = Policy_net('old_policy', env, obs=[obs])\n",
    "\n",
    "            act, _ = Old_Policy.act(stochastic=True)\n",
    "\n",
    "            # Convertir de tensor a array\n",
    "            if type(act)=='Tensor':\n",
    "                # Crear una sesión de TensorFlow\n",
    "                sess = tf.compat.v1.Session()\n",
    "\n",
    "                # Evaluar el tensor dentro de la sesión y obtener el resultado como un objeto NumPy ndarray\n",
    "                act = sess.run(act)\n",
    "\n",
    "                # Cerrar la sesión\n",
    "                sess.close()\n",
    "\n",
    "            if isinstance(act, tf.Tensor):\n",
    "                act = act.numpy()\n",
    "\n",
    "            elif isinstance(act, np.ndarray):\n",
    "                act = act\n",
    "\n",
    "            action = int(act)\n",
    "\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            # --Actualización de variables: ojo no introduzco el estado y accion inicial, solo introduzco los de PPO\n",
    "            observations.append(next_obs)  # S_0\n",
    "\n",
    "            # tenemos una política entrenada\n",
    "            Policy = Policy_net('policy', env, obs=[next_obs])\n",
    "\n",
    "            # Por cada steps en cada episodio, mientras no se llegue a un\n",
    "            # estado terminal o un estado malo\n",
    "            while terminated != True and truncated != True:\n",
    "                # --Aumentar el numero de steps\n",
    "                run_policy_steps += 1\n",
    "\n",
    "                # --Política para ver la acción asociada al estado\n",
    "                # Las observaciones son un de la forma [[s_0,s_1,s_2,....]] por eso su tamaño es (1,n)\n",
    "                action, _ = Policy.get_model().predict(next_obs)\n",
    "\n",
    "                action = int(action)\n",
    "\n",
    "                # --Muevo al Agente al siguiente estado\n",
    "                next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "                # --Actualización de variables\n",
    "                actions.append(action)  # A_i-1\n",
    "                rewards.append(reward)  # R_i-1\n",
    "\n",
    "                # --Si llegamos a un estado final, el juego ha finalizado!!!\n",
    "                # --Se configura el tablero de nuevo\n",
    "                if terminated or truncated:\n",
    "                    obs,_= env.reset()\n",
    "                    reward = -1\n",
    "                    break\n",
    "                else:\n",
    "                    observations.append(next_obs)  # S_i\n",
    "                    self.obs = next_obs\n",
    "\n",
    "            # Ver si el episodio ha obtendo una recompensa total igual o superior a 195\n",
    "            if sum(rewards) >= 195:\n",
    "                success_num += 1\n",
    "                if success_num >= 100:\n",
    "                    break\n",
    "            else:\n",
    "                success_num = 0\n",
    "\n",
    "        observations = np.reshape(observations,newshape=[-1] + list(ob_space.shape))\n",
    "\n",
    "        actions = np.array(actions).astype(dtype=np.int32)\n",
    "\n",
    "        return observations, actions, rewards, Old_Policy, Policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "# Clase PPOTrain\n",
    "##########################################################################################################\n",
    "# Tenemos dos politica theta_i y theta_i+1\n",
    "# Almacenamos dos políticas Policy_net(cada una de ella con su PPO) y calculamos el valor gaes a partir de valores gamma, clip_value, c_1, c_2\n",
    "# Realizamos aqui el entrenamiento, cálculo de gradiente y función de pérdida del PPO para después usarlo en el generador de la GAN\n",
    "class PPOTrain:\n",
    "     def __init__(self,Policy,Old_Policy,obs,actions,rewards,gamma=0.95):\n",
    "        \"\"\"\n",
    "        arg:\n",
    "            Policy\n",
    "            Old_Policy\n",
    "            gamma\n",
    "            clip_value\n",
    "            c_1 parámetro para la diferencia de valores\n",
    "            c_2 parámetro para el bonus de entropía\n",
    "        \"\"\"\n",
    "        self.Policy = Policy\n",
    "        self.Old_Policy = Old_Policy\n",
    "        self.gamma = gamma\n",
    "        self.obs = obs\n",
    "\n",
    "        self.pi_trainable = self.Policy.get_trainable_variables()\n",
    "        self.old_pi_trainable = self.Old_Policy.get_trainable_variables()\n",
    "\n",
    "        policy_dict_ = self.pi_trainable[\"policy\"]\n",
    "        old_policy_dict_ = self.old_pi_trainable[\"policy\"]\n",
    "\n",
    "        self.pi = []\n",
    "        if \"policy\" in self.pi_trainable and  \"policy\" in self.old_pi_trainable:\n",
    "            for param_name, param_value in policy_dict_.items():\n",
    "                # Elimino los pesos que hay en old_policy\n",
    "                del old_policy_dict_[param_name]\n",
    "                # Introduzco los pesos de old_policy en policy\n",
    "                old_policy_dict_[param_name] = param_value\n",
    "                self.pi.append(param_value)\n",
    "        else:\n",
    "            print(\"No se encontró la política con el nombre: policy\")\n",
    "\n",
    "        # Le asignamos old_pi_trainable=pi_trainable ya que ajustaremos unos\n",
    "        # nuevos pi_trainable\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        self.v_preds = self.Old_Policy.get_v_preds()\n",
    "        self.v_preds_next = self.Policy.get_v_preds()\n",
    "        \n",
    "        #  La estimación de la función ventaja GAES, ver paper PPO\n",
    "        self.gaes = self.get_gaes(self.rewards,self.v_preds,self.v_preds_next)\n",
    "\n",
    "        act_probs = self.Policy.get_action_prob()\n",
    "        act_probs_old = self.Old_Policy.get_action_prob()\n",
    "\n",
    "        # la probabilidad de las acciones del agente cuando toma la actual política\n",
    "        act_probs = act_probs * tf.one_hot(indices=self.actions, depth=act_probs.shape[1])\n",
    "        self.act_probs = tf.reduce_sum(act_probs, axis=1)\n",
    "\n",
    "        # la probabilidad de las acciones del agente cuando toma la antigua política\n",
    "        act_probs_old = act_probs_old * tf.one_hot(indices=self.actions, depth=act_probs_old.shape[1])\n",
    "        self.act_probs_old = tf.reduce_sum(act_probs_old, axis=1)\n",
    "\n",
    "        self.loss = loss_fn_ppo( self.act_probs, self.act_probs_old, self.gaes)\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "\n",
    "    def loss_fn_G(self):\n",
    "        return loss_fn_ppo(self.act_probs, self.act_probs_old, self.gaes)\n",
    "\n",
    "    def get_pi_trainable(self):\n",
    "        return self.pi\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        return self.optimizer\n",
    "\n",
    "    def get_OldPolicy(self):\n",
    "        return self.Old_Policy\n",
    "\n",
    "    def get_Policy(self):\n",
    "        return self.Policy\n",
    "\n",
    "    def get_gaes(self, rewards, v_preds, v_preds_next):\n",
    "        deltas = [r_t + self.gamma *v_next - v for r_t, v_next, v in zip(rewards,v_preds_next,v_preds)]\n",
    "        # calcular la estimación generative advantage (lambda = 1), ver ppo paper eq(11)\n",
    "        gaes = copy.deepcopy(deltas)\n",
    "        for t in reversed(range(len(gaes) -1)):  # es T-1, donde T es time step con el que se ejecuta la política\n",
    "            gaes[t] = gaes[t] + self.gamma * gaes[t + 1]\n",
    "        return gaes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# CLASE GAIL\n",
    "##########################################################################\n",
    "\n",
    "class GAN(keras.Model):\n",
    "    # Constructor\n",
    "    def __init__(self, discriminator, generator):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.i=0\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    # Compila el modelo GAN inicializando los optimizadores y la función de pérdida del modelo GAN\n",
    "    def compile(self, d_optimizer, loss_fn_D):\n",
    "        super(GAN, self).compile(run_eagerly=True)\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.loss_fn_D = loss_fn_D\n",
    "\n",
    "    # Devuelve las métricas obtenidas con el generador y discriminador\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    # Evaluación del Discriminador\n",
    "    def evaluate_D(self, X_test):\n",
    "    \n",
    "        len_real = X_test.shape[0]\n",
    "\n",
    "        # Generamos datos falsos dataset_gen = [s,a]\n",
    "        generate_observations, generate_actions, rewards, Old_Policy, Policy = self.generator.generate_fakes()\n",
    "\n",
    "        generate_a_one_hot = np.eye(env.action_space.n)[generate_actions]\n",
    "\n",
    "        dataset_gen = np.concatenate([generate_observations, generate_a_one_hot], axis=1)\n",
    "\n",
    "\n",
    "        dataset_gen = dataset_gen.astype('float32')\n",
    "        dataset_gen[-ac_space.n:] = dataset_gen[-ac_space.n:].astype('int32')\n",
    "        dataset_gen= tf.convert_to_tensor(dataset_gen)\n",
    "        \n",
    "        X_test = X_test.astype('float32')\n",
    "        X_test[-ac_space.n:] = X_test[-ac_space.n:].astype('int32')\n",
    "\n",
    "    \n",
    "        len_fakes = dataset_gen.shape[0]\n",
    "\n",
    "\n",
    "        # Compilamos el discriminador como CNN\n",
    "        discriminator_net.compile( optimizer=tf.keras.optimizers.Adam( learning_rate=1e-5), loss=loss_fn_D, metrics=['accuracy'])\n",
    "\n",
    "        #Evaluamos como CNN\n",
    "        loss_real, acc_real = discriminator_net.evaluate(X_test, tf.ones((len_real,1)), batch_size=len_real, verbose=1)\n",
    "\n",
    "        loss_fake, acc_fake =discriminator_net.evaluate(dataset_gen, tf.ones((len_fakes, 1)), batch_size=len_fakes, verbose=1)\n",
    "\n",
    "        print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real * 100, acc_fake * 100))\n",
    "        print('>Loss real: ')\n",
    "        \n",
    "        print(loss_real)\n",
    "        print('>Loss fake: ')\n",
    "        print(loss_fake)\n",
    "\n",
    "    \n",
    "    # Evaluación del generador\n",
    "\n",
    "    def evaluate_G(self):\n",
    "        # Definimos el entorno\n",
    "        env = gym.make('Eplus-warehouse-hot-discrete-v1')\n",
    "        env = LoggerWrapper(env)\n",
    "\n",
    "        # NUESTRO OBJETIVO: Agente aprenda a tomar las acciones que maximicen\n",
    "        # la recompensa\n",
    "\n",
    "        # Lista donde amacenaremos la recompensa acumulada de cada episodio.\n",
    "        rewards = []\n",
    "\n",
    "        # Para cada episodio, el Agente se mueve por el Entorno mediante acciones hasta llegar a un estado final\n",
    "        # siguiendo la política que se ha aprendido en el entrenamiento de la\n",
    "        # GAN\n",
    "        for episode in range(EPISODES_EVALUATE_G):\n",
    "            truncated = False\n",
    "            terminated = False\n",
    "            R = 0.0\n",
    "            reward = 0.0\n",
    "\n",
    "            # Estado inicial del juego\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "            # Interactuamos con el Entorno hasta que lleguemos a un estado\n",
    "            # final\n",
    "            while terminated != True and truncated != True:\n",
    "                action, _ = self.generator.get_model().predict(obs)\n",
    "                obs, reward, terminated, truncated, _ = env.step(int(action))\n",
    "\n",
    "                # Incremento la recompensa del episodio i al haber ejecutado el step\n",
    "                R += reward\n",
    "\n",
    "            rewards.append(R)\n",
    "\n",
    "            # Vemos para el episodio, su recompensa acumulada que es lo que se\n",
    "            # trata de maximizar\n",
    "            print(\"Episode  {} Total reward: {}\".format(episode, R))\n",
    "\n",
    "        # Cierro el entorno\n",
    "        env.close()\n",
    "\n",
    "        # Muestro las recompensas obtenidas en cada episodio\n",
    "        indices = range(0, EPISODES_EVALUATE_G)\n",
    "        plt.plot(indices, rewards)\n",
    "        plt.show()\n",
    "\n",
    "        return np.mean(rewards)\n",
    "    \n",
    "    def train_step(self, X_train):\n",
    "        # 1) Generamos secuencias falsas [s,a]\n",
    "        generate_observations, generate_actions, rewards, Old_Policy, Policy = self.generator.generate_fakes()\n",
    "\n",
    "        generate_a_one_hot = np.eye(env.action_space.n)[generate_actions]\n",
    "\n",
    "        if generate_observations.shape[0] == generate_a_one_hot.shape[0]:\n",
    "            dataset_gen = np.concatenate([generate_observations, generate_a_one_hot], axis=1)\n",
    "        else:\n",
    "            generate_a_one_hot_resized = np.resize(generate_a_one_hot, generate_observations.shape)\n",
    "            dataset_gen = np.concatenate([generate_observations, generate_a_one_hot_resized], axis=1)\n",
    "\n",
    "    \n",
    "        dataset_gen = dataset_gen.astype('float32')\n",
    "        dataset_gen[-10:] = dataset_gen[-10:].astype(int)\n",
    "\n",
    "        dataset_gen= tf.convert_to_tensor(dataset_gen)\n",
    "\n",
    "        # 2) Obtenemos las secuencias reales [s,a] de los datos de entrenamiento\n",
    "        X_train = tf.strings.to_number(X_train)\n",
    "        X_train = tf.cast(X_train, dtype=tf.float32)\n",
    "\n",
    "        combined_images = tf.concat([X_train, dataset_gen], axis=0)\n",
    "        \n",
    "        # 3) Longitud de datos pueden ser diferente en secuencias reales y secuencias falsas. \n",
    "        # Tener en cuenta que no tenemos la misma cantidad de datos verdaderos y falsos, por eso calculamos len_real y len_fakes\n",
    "        # No podemos controlar la creación de x secuencias [s,a, s', r] ya que generaremos tantas secuencias como se necesiten para finalizar el juego\n",
    "        len_fakes = dataset_gen.shape[0]\n",
    "        len_real = X_train.shape[0] \n",
    "\n",
    "        # 4) Las etiquetas de las imagenes combinadas las tenemos que crear nosotros introduciendo algo de ruido con tf.random.uniform\n",
    "        labels = tf.concat([tf.ones((len_real, 1)), tf.zeros((len_fakes, 1))],axis=0)\n",
    "\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        ##############################################################################################################################################################\n",
    "        # PASO 1:  ENTRENAMIENTO DEL DISCRIMINADOR \n",
    "        #############################################################################################################################################################\n",
    "\n",
    "        # Entrenamiento del discriminador con las [s,a] del agente (falsas o sintéticas) y del experto (reales) combinadas, esto es,\n",
    "        # le pasamos un conjunto que tiene tanto trayectorias reales como trayectorias falsas\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = np.zeros((len_real + len_fakes, ob_space.shape[0] + ac_space.n))\n",
    "            # Predicciones obtenidas con el discriminador\n",
    "            predictions = self.discriminator.discriminator_net(combined_images)\n",
    "            # Valor de la función de pérdida al comparar las predicciones con las etiquetas reales\n",
    "            d_loss = self.loss_fn_D(labels, predictions)\n",
    "\n",
    "        # Calculo del gradiente y actualización del gradiente\n",
    "        grads = tape.gradient(d_loss, self.discriminator.getNet().trainable_weights)\n",
    "\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.getNet().trainable_weights)\n",
    "        )\n",
    "\n",
    "        ################################################################################################################################################################\n",
    "        # PASO 2: ENTRENAMIENTO DEL GENERADOR=POLÍTICA \n",
    "        ######################################################################################################################\n",
    "\n",
    "        ppotrain = PPOTrain(Policy,Old_Policy,actions=generate_actions,rewards=rewards,obs=generate_observations[0])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            g_loss = ppotrain.loss_fn_G()\n",
    "\n",
    "        g_loss = tf.cast(g_loss, dtype=tf.float32)\n",
    "\n",
    "        #################################################################################################################################################################\n",
    "        # PASO 3: Cada 10 épocas, ejecutamos al generador (agente) sobre el entorno. Esto es, llamamos al método evaluate_G.  \n",
    "        #################################################################################################################################################################\n",
    "        # Por cada 10 epoca quiero conocer los datos de recompensa\n",
    "        if (self.i+1)%10==0:\n",
    "          rewardMean=self.evaluate_G()\n",
    "          print('\\nEpoca:', self.i+1)\n",
    "          print('\\nRecompensa de Media:', rewardMean, '\\n')\n",
    "\n",
    "        self.i=self.i+1\n",
    "\n",
    "        ############################################################################################################################################################\n",
    "\n",
    "        # Actualización de métricas del discriminador y generador\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "\n",
    "        return {\"d_loss\": self.d_loss_metric.result(),\n",
    "                \"g_loss\": self.g_loss_metric.result()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de datos experta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# LECTURA DEL DATASET REAL\n",
    "##########################################################################\n",
    "with open('observations_Warehouse.csv', 'r') as archivo_csv:\n",
    "    # Crear el lector CSV\n",
    "    expert_observations = csv.reader(archivo_csv)\n",
    "    with open('actions_Warehouse.csv', 'r') as archivo_csv:\n",
    "        expert_actions = csv.reader(archivo_csv)\n",
    "\n",
    "        # Construimos el dataset [s,a] reales y lo dividimos en training y test\n",
    "        expert_actions = [int(item[0]) for item in list(expert_actions)][:]\n",
    "        expert_a_one_hot = np.eye(env.action_space.n)[expert_actions]\n",
    "        expert_observations = list(expert_observations)\n",
    "        expert_observations = np.array(expert_observations)\n",
    "        dataset = np.concatenate([expert_observations, expert_a_one_hot], axis=1)\n",
    "\n",
    "        print('\\n Tamanio del dataset: ', dataset.shape)\n",
    "        print('\\n Primera fila del dataset: ', dataset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# DIVISIÓN TRAIN Y TEST\n",
    "##############################################################################\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test = train_test_split(dataset, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Imprime el número de elementos en el conjuntos de entrenamiento y prueba\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de entrenamiento:', len(X_train))\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de prueba:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTACIÓN DE GAIL CON WAREHOUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de generador y discriminador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset(seed=1)\n",
    "\n",
    "# Generador\n",
    "generator = Policy_net('policy', env, obs)\n",
    "\n",
    "# Generamos [s,a]=[observations,actions] falsas y las políticas theta_i=Old_Policy y theta_i+1=Policy\n",
    "observations, actions, rewards, Old_Policy, Policy = generator.generate_fakes()\n",
    "\n",
    "# Discriminador\n",
    "discriminator = Discriminator(env,discriminator_net,expert_observations,expert_actions,observations,actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 1: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de Extended GAIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(discriminator=discriminator, generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilación de  GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Compilación \n",
    "gan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss_fn_D=loss_fn_D\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deshabilitar los mensajes de información de TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Deshabilitar los mensajes de información de OpenAI Gym\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "# Entrenamiento GAIL\n",
    "history = gan.fit(X_train,epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.evaluate_D(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Experimentos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias necesarias para la visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[10,20,30,40,50,60,70,80,90,100]\n",
    "y=[]\n",
    "\n",
    "df = pd.DataFrame({'x': x, 'y': y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráfica de Recompensa Media/ Épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, label='Warehouse',color='green')\n",
    "\n",
    "#Agregamos las etiquetas y añadimos una leyenda.\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Recompensa media')\n",
    "plt.title(\"Warehouse\")\n",
    "# plt.legend()\n",
    "plt.savefig('grafica_Warehouse_Extended_Gail.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimación estadística: Modelo de Regresión Lineal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=df, x=\"x\", y=\"y\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
